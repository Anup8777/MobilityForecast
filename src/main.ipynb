{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import PIL\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "# import tensorflow.contrib as tf_contrib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A simplified attention block\n",
    "def hw_flatten(x) :\n",
    "    # return tf.reshape(x, shape=[x.shape[0], -1, x.shape[-1]])\n",
    "    return tf.reshape(x, shape=(x.shape[0], -1, x.shape[-1]))\n",
    "\n",
    "def attention(x, channels=265):\n",
    "\n",
    "    f = tf.keras.layers.Conv2D(filters=1, kernel_size=1, strides=1, padding='same', use_bias=True)(x) # [bs, h, w, c']\n",
    "    g = tf.keras.layers.Conv2D(filters=1, kernel_size=1, strides=1, padding='same', use_bias=True)(x) # [bs, h, w, c']\n",
    "    h = tf.keras.layers.Conv2D(filters=1, kernel_size=1, strides=1, padding='same', use_bias=True)(x) # [bs, h, w, c]\n",
    "    # print('h', h.shape)\n",
    "    # N = h * w\n",
    "    # s = tf.linalg.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True) # # [bs, N, N]\n",
    "    s = tf.matmul(g, f, transpose_b=True) # # [bs, N, N]\n",
    "    # s = tf.matmul(tf.keras.layers.Flatten()(g), tf.keras.layers.Flatten()(f), transpose_b=True) # # [bs, N, N]\n",
    "    # print('s', s.shape)\n",
    "    beta = tf.nn.softmax(s)  # attention map\n",
    "    # print('beta', beta.shape)\n",
    "\n",
    "    # o = tf.linalg.matmul(beta, hw_flatten(h)) # [bs, N, C]\n",
    "    o = tf.matmul(beta, h) # [bs, N, C]\n",
    "    # o = tf.linalg.matmul(beta, tf.keras.layers.Flatten()(h)) # [bs, N, C]\n",
    "    # print('o', o.shape)\n",
    "    # Unsure if this is correct, see documentation: https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable#migrate-to-tf2\n",
    "    gamma = tf.compat.v1.get_variable(\"gamma\", [1], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # o = tf.reshape(o, shape=x.shape) # [bs, h, w, C]\n",
    "    o = tf.keras.layers.Conv2D(filters=channels, kernel_size=1, strides=1, padding='same', use_bias=True)(o)\n",
    "\n",
    "    x = gamma * o + x\n",
    "\n",
    "    return x\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon # this formula is considered best practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv3D_Block(inp_shape):\n",
    "        inp = tf.keras.layers.Input(shape=inp_shape)\n",
    "\n",
    "        # We will construct 4 `ConvLSTM2D` layers with batch normalization,\n",
    "        # followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
    "        x = tf.keras.layers.ConvLSTM2D(filters=32, kernel_size=(3), padding=\"same\", return_sequences=True, activation=\"relu\")(inp)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ConvLSTM2D(filters=16, kernel_size=(3), padding=\"same\", return_sequences=True, activation=\"relu\")(inp)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ConvLSTM2D(filters=8, kernel_size=(3), padding=\"same\", return_sequences=True, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ConvLSTM2D(filters=4, kernel_size=(3), padding=\"same\", return_sequences=True, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        res1 = tf.keras.layers.Conv3D(filters=1, kernel_size=(1,1,1), padding=\"same\")(x)\n",
    "        res1 = tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(res1)\n",
    "        res1 = tf.keras.layers.LeakyReLU(alpha=0.05)(res1)\n",
    "        res1 = tf.keras.layers.BatchNormalization()(res1)\n",
    "        # attention\n",
    "        x = attention(res1, channels=265)\n",
    "        # residual\n",
    "        x = tf.keras.layers.Add()([res1, x])\n",
    "        x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(16,activation='relu')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.05)(x)\n",
    "\n",
    "        # Next, we will build the complete model and compile it.\n",
    "        model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model(latent_dim, batch_size):\n",
    "    \"\"\" Adapted from Laurence Moroney's Coursera course on VAEs: https://www.coursera.org/lecture/generative-deep-learning-with-tensorflow/sampling-layer-and-encoder-G2mJr\"\"\"\n",
    "    demand_model = Conv3D_Block((batch_size, 265,265,1))\n",
    "    ex_f_model = Conv3D_Block((batch_size, 265,265,1))\n",
    "    combined = tf.keras.layers.concatenate([demand_model.output, ex_f_model.output], axis=-1)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(combined)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    z_mean = tf.keras.layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = tf.keras.layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = tf.keras.Model(inputs=[demand_model.input, ex_f_model.input], outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(latent_dim):\n",
    "    latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(latent_inputs)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Reshape(target_shape=(4, 4, 4))(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=4, kernel_size=(3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=(3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=(3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Reshape(target_shape=(4, 4, 32,1))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters=1, kernel_size=(3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.05)(x)\n",
    "    \n",
    "    #generator = tf.keras.Model(latent_inputs, outputs=[x_real, x_enc, x_fake], name=\"generator\")\n",
    "    generator = tf.keras.Model(latent_inputs, outputs=x, name=\"generator\")\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, generator, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return[self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "    def train_step(self,data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_variance, z = self.encoder([data[0], data[1]])\n",
    "            reconstruction = self.generator(z)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1,2)))\n",
    "            kl_loss = -0.5 * (1 + z_log_variance - tf.square(z_mean) - tf.exp(z_log_variance))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss =reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return { \n",
    "            \"loss\":self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 4\n",
    "batch_size = 1 # Paper: 32\n",
    "encoder = encoder_model(latent_dim, batch_size)\n",
    "generator = generator_model(latent_dim)\n",
    "vae = CVAE(encoder, generator)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.random.normal(shape = (1,batch_size,265, 265, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\20215176\\AppData\\Local\\Temp\\ipykernel_8100\\2253143406.py\", line 16, in train_step\n        z_mean, z_log_variance, z = self.encoder([data[0], data[1]])\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 296, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"encoder\" is incompatible with the layer: expected shape=(None, 1, 265, 265, 1), found shape=(1, 265, 265, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8100\\3629043932.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8100\\2253143406.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mz_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_log_variance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mreconstruction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mreconstruction_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\20215176\\AppData\\Local\\Temp\\ipykernel_8100\\2253143406.py\", line 16, in train_step\n        z_mean, z_log_variance, z = self.encoder([data[0], data[1]])\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\20215176\\Anaconda3\\envs\\mobilityforecast\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 296, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"encoder\" is incompatible with the layer: expected shape=(None, 1, 265, 265, 1), found shape=(1, 265, 265, 1)\n"
     ]
    }
   ],
   "source": [
    "vae.fit(train_data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_variance, z = encoder([train_data, train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits= generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_91 (InputLayer)          [(None, 1, 265, 265  0           []                               \n",
      "                                , 1)]                                                             \n",
      "                                                                                                  \n",
      " input_92 (InputLayer)          [(None, 1, 265, 265  0           []                               \n",
      "                                , 1)]                                                             \n",
      "                                                                                                  \n",
      " conv_lstm2d_274 (ConvLSTM2D)   (None, 1, 265, 265,  9856        ['input_91[0][0]']               \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " conv_lstm2d_278 (ConvLSTM2D)   (None, 1, 265, 265,  9856        ['input_92[0][0]']               \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_340 (Batch  (None, 1, 265, 265,  64         ['conv_lstm2d_274[0][0]']        \n",
      " Normalization)                  16)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_345 (Batch  (None, 1, 265, 265,  64         ['conv_lstm2d_278[0][0]']        \n",
      " Normalization)                  16)                                                              \n",
      "                                                                                                  \n",
      " conv_lstm2d_275 (ConvLSTM2D)   (None, 1, 265, 265,  6944        ['batch_normalization_340[0][0]']\n",
      "                                 8)                                                               \n",
      "                                                                                                  \n",
      " conv_lstm2d_279 (ConvLSTM2D)   (None, 1, 265, 265,  6944        ['batch_normalization_345[0][0]']\n",
      "                                 8)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_341 (Batch  (None, 1, 265, 265,  32         ['conv_lstm2d_275[0][0]']        \n",
      " Normalization)                  8)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_346 (Batch  (None, 1, 265, 265,  32         ['conv_lstm2d_279[0][0]']        \n",
      " Normalization)                  8)                                                               \n",
      "                                                                                                  \n",
      " conv_lstm2d_276 (ConvLSTM2D)   (None, 1, 265, 265,  1744        ['batch_normalization_341[0][0]']\n",
      "                                 4)                                                               \n",
      "                                                                                                  \n",
      " conv_lstm2d_280 (ConvLSTM2D)   (None, 1, 265, 265,  1744        ['batch_normalization_346[0][0]']\n",
      "                                 4)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_342 (Batch  (None, 1, 265, 265,  16         ['conv_lstm2d_276[0][0]']        \n",
      " Normalization)                  4)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_347 (Batch  (None, 1, 265, 265,  16         ['conv_lstm2d_280[0][0]']        \n",
      " Normalization)                  4)                                                               \n",
      "                                                                                                  \n",
      " conv3d_67 (Conv3D)             (None, 1, 265, 265,  5           ['batch_normalization_342[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " conv3d_68 (Conv3D)             (None, 1, 265, 265,  5           ['batch_normalization_347[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d_67 (MaxPooling3D  (None, 1, 132, 132,  0          ['conv3d_67[0][0]']              \n",
      " )                               1)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d_68 (MaxPooling3D  (None, 1, 132, 132,  0          ['conv3d_68[0][0]']              \n",
      " )                               1)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_134 (LeakyReLU)    (None, 1, 132, 132,  0           ['max_pooling3d_67[0][0]']       \n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_136 (LeakyReLU)    (None, 1, 132, 132,  0           ['max_pooling3d_68[0][0]']       \n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_343 (Batch  (None, 1, 132, 132,  4          ['leaky_re_lu_134[0][0]']        \n",
      " Normalization)                  1)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_348 (Batch  (None, 1, 132, 132,  4          ['leaky_re_lu_136[0][0]']        \n",
      " Normalization)                  1)                                                               \n",
      "                                                                                                  \n",
      " conv2d_235 (Conv2D)            (None, 1, 132, 132,  2           ['batch_normalization_343[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " conv2d_234 (Conv2D)            (None, 1, 132, 132,  2           ['batch_normalization_343[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " conv2d_239 (Conv2D)            (None, 1, 132, 132,  2           ['batch_normalization_348[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " conv2d_238 (Conv2D)            (None, 1, 132, 132,  2           ['batch_normalization_348[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_116 (TFOpLamb  (None, 1, 132, 132,  0          ['conv2d_235[0][0]',             \n",
      " da)                             132)                             'conv2d_234[0][0]']             \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_118 (TFOpLamb  (None, 1, 132, 132,  0          ['conv2d_239[0][0]',             \n",
      " da)                             132)                             'conv2d_238[0][0]']             \n",
      "                                                                                                  \n",
      " tf.nn.softmax_58 (TFOpLambda)  (None, 1, 132, 132,  0           ['tf.linalg.matmul_116[0][0]']   \n",
      "                                 132)                                                             \n",
      "                                                                                                  \n",
      " conv2d_236 (Conv2D)            (None, 1, 132, 132,  2           ['batch_normalization_343[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " tf.nn.softmax_59 (TFOpLambda)  (None, 1, 132, 132,  0           ['tf.linalg.matmul_118[0][0]']   \n",
      "                                 132)                                                             \n",
      "                                                                                                  \n",
      " conv2d_240 (Conv2D)            (None, 1, 132, 132,  2           ['batch_normalization_348[0][0]']\n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_117 (TFOpLamb  (None, 1, 132, 132,  0          ['tf.nn.softmax_58[0][0]',       \n",
      " da)                             1)                               'conv2d_236[0][0]']             \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_119 (TFOpLamb  (None, 1, 132, 132,  0          ['tf.nn.softmax_59[0][0]',       \n",
      " da)                             1)                               'conv2d_240[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_237 (Conv2D)            (None, 1, 132, 132,  530         ['tf.linalg.matmul_117[0][0]']   \n",
      "                                 265)                                                             \n",
      "                                                                                                  \n",
      " conv2d_241 (Conv2D)            (None, 1, 132, 132,  530         ['tf.linalg.matmul_119[0][0]']   \n",
      "                                 265)                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_58 (TFOpLambd  (None, 1, 132, 132,  0          ['conv2d_237[0][0]']             \n",
      " a)                              265)                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_59 (TFOpLambd  (None, 1, 132, 132,  0          ['conv2d_241[0][0]']             \n",
      " a)                              265)                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_58 (TFOpL  (None, 1, 132, 132,  0          ['tf.math.multiply_58[0][0]',    \n",
      " ambda)                          265)                             'batch_normalization_343[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_59 (TFOpL  (None, 1, 132, 132,  0          ['tf.math.multiply_59[0][0]',    \n",
      " ambda)                          265)                             'batch_normalization_348[0][0]']\n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 1, 132, 132,  0           ['batch_normalization_343[0][0]',\n",
      "                                 265)                             'tf.__operators__.add_58[0][0]']\n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 1, 132, 132,  0           ['batch_normalization_348[0][0]',\n",
      "                                 265)                             'tf.__operators__.add_59[0][0]']\n",
      "                                                                                                  \n",
      " dense_350 (Dense)              (None, 1, 132, 132,  17024       ['add_58[0][0]']                 \n",
      "                                 64)                                                              \n",
      "                                                                                                  \n",
      " dense_353 (Dense)              (None, 1, 132, 132,  17024       ['add_59[0][0]']                 \n",
      "                                 64)                                                              \n",
      "                                                                                                  \n",
      " dense_351 (Dense)              (None, 1, 132, 132,  2080        ['dense_350[0][0]']              \n",
      "                                 32)                                                              \n",
      "                                                                                                  \n",
      " dense_354 (Dense)              (None, 1, 132, 132,  2080        ['dense_353[0][0]']              \n",
      "                                 32)                                                              \n",
      "                                                                                                  \n",
      " dense_352 (Dense)              (None, 1, 132, 132,  528         ['dense_351[0][0]']              \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " dense_355 (Dense)              (None, 1, 132, 132,  528         ['dense_354[0][0]']              \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_135 (LeakyReLU)    (None, 1, 132, 132,  0           ['dense_352[0][0]']              \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_137 (LeakyReLU)    (None, 1, 132, 132,  0           ['dense_355[0][0]']              \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 1, 132, 132,  0           ['leaky_re_lu_135[0][0]',        \n",
      "                                 32)                              'leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " dense_356 (Dense)              (None, 1, 132, 132,  2112        ['concatenate_29[0][0]']         \n",
      "                                 64)                                                              \n",
      "                                                                                                  \n",
      " dense_357 (Dense)              (None, 1, 132, 132,  2080        ['dense_356[0][0]']              \n",
      "                                 32)                                                              \n",
      "                                                                                                  \n",
      " dense_358 (Dense)              (None, 1, 132, 132,  528         ['dense_357[0][0]']              \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " flatten_30 (Flatten)           (None, 278784)       0           ['dense_358[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 4)            1115140     ['flatten_30[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 4)            1115140     ['flatten_30[0][0]']             \n",
      "                                                                                                  \n",
      " sampling_29 (Sampling)         (None, 4)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,312,666\n",
      "Trainable params: 2,312,550\n",
      "Non-trainable params: 116\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mobilityforecast')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a489ebabc6b1c3c996df92d9378f45c983839505199bbf18af484bb22de14d32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
